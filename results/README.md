# Assessment Results

This directory contains assessment results generated by automated evaluation workflows.

## Result Format

Each result file is a JSON file containing assessment outcomes from the green agent (evaluator):

- `participants`: Agent identifiers and roles
- `results[]`: Array of evaluation results for subtasks
  - `subtask`: Either "subtask1" (Clinical Decision Making) or "subtask2" (Confabulation Detection)
  - `total_tasks`: Number of tasks evaluated
  - `correct_tasks`: Number of correct responses
  - `accuracy`: Accuracy score (0.0 to 1.0)
  - `hallucination_rate`: Only for subtask2 (lower is better)
  - `timestamp`: ISO timestamp of evaluation

## File Naming Convention

Results are automatically named: `assessment_{timestamp}.json` by the GitHub Actions workflow.

## Manual Generation

For manual testing, results can be generated using the `submission.py` script:

```bash
# Generate result file
python ../submission.py evaluation_output.json participant_123 --save results/manual_assessment.json
```